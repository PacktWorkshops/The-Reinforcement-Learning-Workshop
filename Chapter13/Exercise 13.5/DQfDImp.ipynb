{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tfc\n",
    "import numpy as np\n",
    "import random\n",
    "import functools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQfDAgent:\n",
    "    def __init__(self, env, configuration,trajectories =None):\n",
    "        self.session = tfc.InteractiveSession()\n",
    "        self.conf = configuration\n",
    "        self.replay = Memory(capacity=self.conf.replay_buffer_size, permanent_data=len(trajectories))\n",
    "        self.demos = Memory(capacity=self.conf.demo_buffer_size, permanent_data=self.conf.demo_buffer_size)\n",
    "        self.include_trajectories(trajectories= trajectories)  # add demo data to both demo_memory & replay_memory\n",
    "        self.step = 0\n",
    "        self.eps = self.conf.eps_init\n",
    "        self.state_no = env.observation_space.shape[0]\n",
    "        self.action_no = env.action_space.n\n",
    "        \n",
    "\n",
    "    #function to include transitions to demonstration memory\n",
    "    def include_trajectories(self, trajectories):\n",
    "        for tra in trajectories :\n",
    "            self.demos.store(np.array(tra, dtype=object))\n",
    "            self.replay.store(np.array(tra, dtype=object))\n",
    "\n",
    "\n",
    "#function for greedy policy\n",
    "    def greedy_act(self, current_state, model):\n",
    "        if np.random.random() < self.eps:\n",
    "            return np.random.randint(0, self.action_no - 1)\n",
    "        return np.argmax(model.predict(current_state)[0])\n",
    "    \n",
    "\n",
    "\n",
    "#function for deep neural network layers\n",
    "    def neural_net_layers(self, no_units1, no_units2, reg=None):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(no_units1, input_dim = self.state_no, kernel_regularizer= reg))\n",
    "        model.add(Dense(no_units2, kernel_regularizer= reg))\n",
    "        model.add(Dense(self.action_no))\n",
    "        model.compile(loss = \"mean_squared_error\", optimizer = Adam(lr = self.conf.alpha))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    #function for selecting network\n",
    "    def select_q(self):\n",
    "        reg = tf.keras.regularizers.l2(l=0.2)  \n",
    "        return self.neural_net_layers(24, 48, reg)\n",
    "       \n",
    "    #function for evaluating the network \n",
    "    def eval_q(self):\n",
    "        return self.neural_net_layers(24, 48)\n",
    "    \n",
    "    def train_network(self, train =False, update=True):\n",
    "       \n",
    "        self.step= self.step + 1\n",
    "\n",
    "        actual_mem = self.demos if train else self.replay\n",
    "        minibatch = actual_mem.sample(self.conf.minibatch)\n",
    "      \n",
    "        np.random.shuffle(minibatch)\n",
    "        current_state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        new_state_batch = [data[3] for data in minibatch]\n",
    "        done_batch = [data[4] for data in minibatch]\n",
    "        demos_data = [data[5] for data in minibatch]\n",
    "        nth_step_reward_batch = [data[6] for data in minibatch]\n",
    "        nth_step_state_batch = [data[7] for data in minibatch]\n",
    "        nth_step_done_batch = [data[8] for data in minibatch]\n",
    "        actual_no = [data[9] for data in minibatch]\n",
    "\n",
    "        # provide for placeholderï¼Œcompute first\n",
    "        select_q = self.select_q()\n",
    "        eval_q = self.eval_q()\n",
    "        n_step_select_q = self.select_q()\n",
    "        n_step_eval_q = self.eval_q()\n",
    "\n",
    "        y_batch = np.zeros((self.conf.minibatch, self.action_no))\n",
    "        n_step_y_batch = np.zeros((self.conf.minibatch, self.action_no))\n",
    "        for i in range(self.conf.minibatch):\n",
    "            temp = select_q.predict(current_state_batch[i].reshape((-1, self.state_no)))[0]\n",
    "            temp_0 = temp\n",
    "            # add 1-step reward\n",
    "            action = self.greedy_act(current_state_batch[i].reshape((-1, self.state_no)), select_q)\n",
    "            new_state_batch = new_state_batch[i].reshape(1,4)\n",
    "            new_q = max(eval_q.predict(new_state_batch)[0])\n",
    "            temp[action_batch[i]] = reward_batch[i] + (1 - int(done_batch[i])) * self.conf.gamma * new_q\n",
    "            y_batch[i] = temp\n",
    "            # add n-step reward\n",
    "            action = self.greedy_act(nth_step_state_batch[i],select_q)\n",
    "            n_step_new_q = max(n_step_eval_q.predict(new_state_batch[i].reshape(1,4))[0])\n",
    "            q_nth_step = (1 - int(nth_step_done_batch[i])) * self.conf.gamma**actual_no[i] *n_step_new_q\n",
    "            temp_0[action_batch[i]] = nth_step_reward_batch[i] + q_nth_step\n",
    "            n_step_y_batch[i] = temp_0\n",
    "        \n",
    "        return y_batch,n_step_y_batch,current_state_batch, action_batch,demos_data\n",
    "\n",
    "\n",
    "    def loss_lin(self, ae, a):\n",
    "        return 0.0 if ae == a else 0.75\n",
    "\n",
    "    def loss_selec(self, select_q, action_batch, demo_data):\n",
    "        inp = 0.0\n",
    "        for i in range(self.conf.minibatch):\n",
    "            ae = action_batch[i]\n",
    "            max_val = float(\"-inf\")\n",
    "            for act in range(self.action_no):\n",
    "                max_val = max(select_q[i][act] + self.loss_lin(ae, act), max_val)\n",
    "            inp += demo_data[i] * (max_val - select_q[i][ae])\n",
    "        return inp\n",
    "\n",
    "   \n",
    "    def loss(self, select_q, y_batch, n_step_y_batch,action_batch,demo_data, weights):\n",
    "        loss_dq = tf.math.reduce_mean(tf.math.squared_difference(select_q, y_batch))\n",
    "        lloss_dq = tf.math.reduce_mean(tf.math.squared_difference(select_q, n_step_y_batch))\n",
    "        loss_inp = self.loss_selec(y_batch, action_batch, demo_data)\n",
    "        loss_l2 = tf.math.reduce_sum([tf.math.reduce_mean(reg_l) for reg_l in tfc.get_collection(tfc.GraphKeys.REGULARIZATION_LOSSES)])\n",
    "        return weights * tf.math.reduce_sum([l * lam for l, lam in zip([loss_dq, lloss_dq, loss_inp, loss_l2], self.conf.lamb)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def perceive(self, trajectory):\n",
    "        self.replay.store(np.array(trajectory))\n",
    "        \n",
    "        if self.replay.full==True:\n",
    "            self.eps = max(self.conf.eps_fin, self.eps * self.conf.decay)\n",
    "\n",
    "    \n",
    "    \n",
    "    #function for training \n",
    "    def train_ahead(self):\n",
    "        for tr in range(self.conf.pretraining):\n",
    "            self.train_network(train=True)\n",
    "            if tr % 200 == 0 and tr > 0:\n",
    "                print('Training step with expert demonstrations: {}'.format(tr))\n",
    "        self.step= 0\n",
    "        print('Finished demo training')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, capacity, permanent_data):\n",
    "        self.capacity = capacity\n",
    "        self.permanent_data = permanent_data\n",
    "        assert 0<=self.permanent_data <= self.capacity\n",
    "        self.mem = []\n",
    "        self.pos = 0\n",
    "        self.full = False\n",
    "    \n",
    "    def store(self, transition):\n",
    "        if len(self.mem) < self.capacity:\n",
    "            self.mem.append(None)\n",
    "        self.mem[self.pos] = transition\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        if self.permanent_data >= self.capacity:\n",
    "            self.full = True\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.mem, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mem)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}