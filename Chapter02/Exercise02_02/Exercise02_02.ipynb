{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving MDPs: Linear Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MDP](../pictures/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to solve the MDP in figure using Linear Programming. In this MDP the environment model is extremely simple, the transition function is deterministic so it is determined uniquely by the action. \n",
    "\n",
    "The variables of the linear program are the state values, the coefficients are given by the initial state distribution, that in our case is a deterministic function, as the state 1 is the initial state.\n",
    "So the coefficients of the objective function are [1, 0, 0].\n",
    "\n",
    "In the following we will use scipy.optimize.linprog function to optimize a linear program.\n",
    "\n",
    "We will use the following notation:\n",
    "\n",
    "![linear-program](../pictures/linprog.png)\n",
    "\n",
    "To rephrase the problem using upper bounds we have:\n",
    "\n",
    "$$\n",
    "V >= R + \\gamma P V\n",
    "$$\n",
    "\n",
    "That becomes:\n",
    "\n",
    "$$\n",
    "(\\gamma P - I)V <= - R\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of states and number of actions\n",
    "n_states = 3\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial state distribution\n",
    "mu = np.array([[1, 0, 0]]).T\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the upper bound coefficients for the action A\n",
    "# define the reward matrix for action A\n",
    "R_A = np.zeros((n_states, 1), np.float)\n",
    "R_A[0, 0] = 1\n",
    "R_A[1, 0] = 0\n",
    "R_A[2, 0] = 0\n",
    "R_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the transition matrix for action A\n",
    "P_A = np.zeros((n_states, n_states), np.float)\n",
    "P_A[0, 1] = 1\n",
    "P_A[1, 0] = 1\n",
    "P_A[2, 1] = 1\n",
    "P_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  0.9,  0. ],\n",
       "       [ 0.9, -1. ,  0. ],\n",
       "       [ 0. ,  0.9, -1. ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper bound A matrix for action A\n",
    "A_up_A = gamma * P_A - np.eye(3,3)\n",
    "A_up_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same for action B\n",
    "# define the reward matrix for action B\n",
    "R_B = np.zeros((n_states, 1), np.float)\n",
    "R_B[0, 0] = 10\n",
    "R_B[1, 0] = 1\n",
    "R_B[2, 0] = 10\n",
    "R_B\n",
    "# Define the transition matrix for action A\n",
    "P_B = np.zeros((n_states, n_states), np.float)\n",
    "P_B[0, 2] = 1\n",
    "P_B[1, 2] = 1\n",
    "P_B[2, 2] = 1\n",
    "P_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1. ,  0. ,  0.9],\n",
       "       [ 0. , -1. ,  0.9],\n",
       "       [ 0. ,  0. , -0.1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper bound A matrix for action B\n",
    "A_up_B = gamma * P_B - np.eye(3,3)\n",
    "A_up_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper bound matrix for all actions and all states\n",
    "A_up = np.vstack((A_up_A, A_up_B))\n",
    "# verify the shape: number of constraints are equal to |actions| * |states|\n",
    "assert(A_up.shape[0] == n_states * n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward vector is obtained by stacking the two vectors\n",
    "R = np.vstack((R_A, R_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mu\n",
    "b_up = -R\n",
    "# Solve the linear program\n",
    "res = scipy.optimize.linprog(c, A_up, b_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the results: state values\n",
    "V_ = res.x\n",
    "V_\n",
    "V = V_.reshape((-1, 1))\n",
    "V\n",
    "np.savetxt(\"solution/V.txt\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![toy-mdp-solved](../pictures/toy-mdp-solved.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the results.\n",
    "We have that the value of state 2 is the lowest one, as expected.\n",
    "The values of states 1 and 3 are very close to each other and approximately equal to 1e+2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the optimal policy by calculating the optimal action value function for each state action couple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transition matrix. On the rows we have states and actions, on the columns we have next states\n",
    "P = np.vstack((P_A, P_B))\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the action value formula to calculate the action values for each state action pair.\n",
    "Q_sa = R + gamma * P.dot(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 88.32127683],\n",
       "       [ 89.99999645],\n",
       "       [ 87.32127683],\n",
       "       [100.00000622],\n",
       "       [ 91.00000622],\n",
       "       [100.00000622]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first three rows are associated to action A, the last three are associated to action B\n",
    "Q_sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape so that it is easier to understand best actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 88.32127683, 100.00000622],\n",
       "       [ 89.99999645,  91.00000622],\n",
       "       [ 87.32127683, 100.00000622]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_sa_2 = np.stack((Q_sa[:3, 0], Q_sa[3:, 0]), axis=1)\n",
    "Q_sa_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actions = np.reshape(np.argmax(Q_sa_2, axis=1), (3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action 1 (B) is the best action in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![toy-mdp-solved](../pictures/toy-mdp-solved_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the best action in state 1 is action B. The action B is the best action for all states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
